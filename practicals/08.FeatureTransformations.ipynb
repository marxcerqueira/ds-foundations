{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning Objectives\n",
    "\n",
    "This should be another short one, but we need to go ahead and do this. I have shown you how to transform qualitative features into quantitative ones, and this is quite useful for computing summary statistics, doing bootstrap and making visualizations, but now we are starting to be interested in prediction. For this we will need to do a slightly different feature transformaiton for qualitative variables and we will need to do some cleaning of quantitative variables too. These steps are outlines below:\n",
    "\n",
    "1. Dummy variable drop one\n",
    "2. Impute or drop Nans\n",
    "3. Standardization\n",
    "\n",
    "Again, before I go on, the above steps are not a complete list of what you could do and are not always needed, which is why I'll try to give some intuition into why we do them and what else can be done:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Dummy Variable drop\n",
    "\n",
    "If you remember from a couple of lessons ago we talked about transforming qualitative variables into quantitative ones by creating an equal number of columns as there were qualitative values. This is a good approach for visualization or trying to come up with confidence intervals for specific features under different conditions, but is not effective for machine learning.\n",
    "\n",
    "In most machine learning algorithms you will need to remove correlated features as they can prove detramental to the results. You can determine which features might be highly correlated by using a correlation matrix, let's use one below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('../data/billionaires.csv')\n",
    "\n",
    "del df['was founder']\n",
    "del df['inherited']\n",
    "del df['from emerging']\n",
    "\n",
    "df.age.replace(-1, np.NaN, inplace=True)\n",
    "df.founded.replace(0, np.NaN, inplace=True)\n",
    "df.gdp.replace(0, np.NaN, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gdp</th>\n",
       "      <th>worth in billions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gdp</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.047724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worth in billions</th>\n",
       "      <td>0.047724</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        gdp  worth in billions\n",
       "gdp                1.000000           0.047724\n",
       "worth in billions  0.047724           1.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['gdp', 'worth in billions']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So notice that the above are not correlated features so you will generally not have to worry about them. That being said, the correlation matrix above will only measure linear correlation between two entities, so you will need to use your head to figure out what is correlated sometimes. And this is all too true when talking about dummy variables.\n",
    "\n",
    "Let's say you have a qualitative feature that is red, green and blue. If you make three columns, these three columns will be completely dependent on each other. Why? Because if I know that the red column is 0 and the green column is 0, then I am completely assured that the blue column is 1. In fact if I know what the green and the red columns are, I will always know what the blue column is. Thus the columns all together are correlated and redundent. We can solve this issue easily by dropping one of the dummy variable columns.\n",
    "\n",
    "I'll show you a quick way to do this below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>category</th>\n",
       "      <th>citizenship</th>\n",
       "      <th>company.name</th>\n",
       "      <th>company.type</th>\n",
       "      <th>country code</th>\n",
       "      <th>founded</th>\n",
       "      <th>gdp</th>\n",
       "      <th>gender</th>\n",
       "      <th>industry</th>\n",
       "      <th>...</th>\n",
       "      <th>region</th>\n",
       "      <th>relationship</th>\n",
       "      <th>sector</th>\n",
       "      <th>was political</th>\n",
       "      <th>worth in billions</th>\n",
       "      <th>year</th>\n",
       "      <th>wealth.type_founder non-finance</th>\n",
       "      <th>wealth.type_inherited</th>\n",
       "      <th>wealth.type_privatized and resources</th>\n",
       "      <th>wealth.type_self-made finance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Financial</td>\n",
       "      <td>Saudi Arabia</td>\n",
       "      <td>Rolaco Trading and Contracting Company</td>\n",
       "      <td>new</td>\n",
       "      <td>SAU</td>\n",
       "      <td>1968.0</td>\n",
       "      <td>1.580000e+11</td>\n",
       "      <td>male</td>\n",
       "      <td>Money Management</td>\n",
       "      <td>...</td>\n",
       "      <td>Middle East/North Africa</td>\n",
       "      <td>founder</td>\n",
       "      <td>construction</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1996</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34.0</td>\n",
       "      <td>Financial</td>\n",
       "      <td>United States</td>\n",
       "      <td>Fidelity Investments</td>\n",
       "      <td>new</td>\n",
       "      <td>USA</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>8.100000e+12</td>\n",
       "      <td>female</td>\n",
       "      <td>Money Management</td>\n",
       "      <td>...</td>\n",
       "      <td>North America</td>\n",
       "      <td>relation</td>\n",
       "      <td>investment banking</td>\n",
       "      <td>False</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1996</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59.0</td>\n",
       "      <td>Non-Traded Sectors</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>Companhia Brasileira de Distribui?ao</td>\n",
       "      <td>new</td>\n",
       "      <td>BRA</td>\n",
       "      <td>1948.0</td>\n",
       "      <td>8.540000e+11</td>\n",
       "      <td>male</td>\n",
       "      <td>Retail, Restaurant</td>\n",
       "      <td>...</td>\n",
       "      <td>Latin America</td>\n",
       "      <td>relation</td>\n",
       "      <td>retail</td>\n",
       "      <td>False</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1996</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61.0</td>\n",
       "      <td>New Sectors</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Ratiopharm</td>\n",
       "      <td>new</td>\n",
       "      <td>DEU</td>\n",
       "      <td>1881.0</td>\n",
       "      <td>2.500000e+12</td>\n",
       "      <td>male</td>\n",
       "      <td>Technology-Medical</td>\n",
       "      <td>...</td>\n",
       "      <td>Europe</td>\n",
       "      <td>relation</td>\n",
       "      <td>pharmaceuticals</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1996</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Financial</td>\n",
       "      <td>Hong Kong</td>\n",
       "      <td>Swire</td>\n",
       "      <td>new</td>\n",
       "      <td>HKG</td>\n",
       "      <td>1816.0</td>\n",
       "      <td>1.600000e+11</td>\n",
       "      <td>male</td>\n",
       "      <td>Money Management</td>\n",
       "      <td>...</td>\n",
       "      <td>East Asia</td>\n",
       "      <td>relation</td>\n",
       "      <td>trading company</td>\n",
       "      <td>False</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1996</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    age            category    citizenship  \\\n",
       "0   NaN           Financial   Saudi Arabia   \n",
       "1  34.0           Financial  United States   \n",
       "2  59.0  Non-Traded Sectors         Brazil   \n",
       "3  61.0         New Sectors        Germany   \n",
       "4   NaN           Financial      Hong Kong   \n",
       "\n",
       "                             company.name company.type country code  founded  \\\n",
       "0  Rolaco Trading and Contracting Company          new          SAU   1968.0   \n",
       "1                    Fidelity Investments          new          USA   1946.0   \n",
       "2    Companhia Brasileira de Distribui?ao          new          BRA   1948.0   \n",
       "3                              Ratiopharm          new          DEU   1881.0   \n",
       "4                                   Swire          new          HKG   1816.0   \n",
       "\n",
       "            gdp  gender            industry              ...                \\\n",
       "0  1.580000e+11    male    Money Management              ...                 \n",
       "1  8.100000e+12  female    Money Management              ...                 \n",
       "2  8.540000e+11    male  Retail, Restaurant              ...                 \n",
       "3  2.500000e+12    male  Technology-Medical              ...                 \n",
       "4  1.600000e+11    male    Money Management              ...                 \n",
       "\n",
       "                     region  relationship              sector was political  \\\n",
       "0  Middle East/North Africa       founder        construction         False   \n",
       "1             North America      relation  investment banking         False   \n",
       "2             Latin America      relation              retail         False   \n",
       "3                    Europe      relation     pharmaceuticals         False   \n",
       "4                 East Asia      relation     trading company         False   \n",
       "\n",
       "  worth in billions  year  wealth.type_founder non-finance  \\\n",
       "0               1.0  1996                                0   \n",
       "1               2.5  1996                                0   \n",
       "2               1.2  1996                                0   \n",
       "3               1.0  1996                                0   \n",
       "4               2.2  1996                                0   \n",
       "\n",
       "   wealth.type_inherited  wealth.type_privatized and resources  \\\n",
       "0                      0                                     0   \n",
       "1                      1                                     0   \n",
       "2                      1                                     0   \n",
       "3                      1                                     0   \n",
       "4                      1                                     0   \n",
       "\n",
       "   wealth.type_self-made finance  \n",
       "0                              1  \n",
       "1                              0  \n",
       "2                              0  \n",
       "3                              0  \n",
       "4                              0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.get_dummies(df, columns=['wealth.type'], drop_first=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Impute or Drop NaNs\n",
    "\n",
    "This is an important step but is often missed. Machine learning algorithms don't know what to do with missing data on their own. The best way to deal with missing data is task dependent. But there are some common strategies that work. One of these strategies is to impute the data points. This is where we try to infer what the missing values were.\n",
    "\n",
    "Some simple imputation strategies would be to take the median or the mean of the data, I'll show you how to do this below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2614 entries, 0 to 2613\n",
      "Data columns (total 1 columns):\n",
      "age    2229 non-null float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 20.5 KB\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "imp = Imputer(strategy='median')\n",
    "\n",
    "df[['age']].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2614, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp.fit_transform(df[['age']]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here we filled all the missing ages with the median age. \n",
    "\n",
    "The second thing that you can do is you can drop missing values. Where you have plenty of data points and the missing values are not correlated, this can work particularly well. I'll show how you do this below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age    2229\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['age']].dropna().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Standardization\n",
    "\n",
    "This is the final common technique that is used. In this technique we normalize the mean and variance of our features. \n",
    "\n",
    "This can be good or bad. Some ML algorithms will work much better with standardized features (especially those with complex learning algorithms). It can be good because certain features have a much bigger scale than others (let's say the gdp above), but this can also be bad at times. Sometimes you will want to retain the original feature values because of interpretability.\n",
    "\n",
    "One of the small things that you will want to do before you do this is remove features of very low variance. \n",
    "\n",
    "I'll show you how to do this below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age\n",
       "1   34.0\n",
       "2   59.0\n",
       "3   61.0\n",
       "8   66.0\n",
       "10  12.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['age']].dropna().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.1761343 ],\n",
       "       [-0.27235416],\n",
       "       [-0.12005174],\n",
       "       [ 0.26070429],\n",
       "       [-3.85146083]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std.fit_transform(df[['age']].dropna())[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now we have some negative ages!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## One final note\n",
    "\n",
    "The preprocessing steps are incredibly important and can be even more important than the algorithm itself. One should really think of the above as part of the machine learning algorithm itself. And this mentality will help a great deal. \n",
    "\n",
    "In fact you will need to train the above on your training data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning Objectives\n",
    "\n",
    "This should be another short one, but we need to go ahead and do this. I have shown you how to transform qualitative features into quantitative ones, and this is quite useful for computing summary statistics, doing bootstrap and making visualizations, but now we are starting to be interested in prediction. For this we will need to do a slightly different feature transformaiton for qualitative variables and we will need to do some cleaning of quantitative variables too. These steps are outlines below:\n",
    "\n",
    "1. Dummy variable drop one\n",
    "2. Impute or drop Nans\n",
    "3. Standardization\n",
    "\n",
    "Again, before I go on, the above steps are not a complete list of what you could do and are not always needed, which is why I'll try to give some intuition into why we do them and what else can be done:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Comprehension Questions\n",
    "\n",
    "1.\tCan other features in your dataset be completely correlated with each other? How can you tell?\n",
    "2.\tWhere do NaNs come from?\n",
    "3.\tWhen does imputing the mean/median not make sense?\n",
    "4.\tWhat else can you do other than impute values in columns that have missing data?\n",
    "5.\tWhich algorithms might you not want standardization?\n",
    "6.\tWhy would standardization help ML algorithms?\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
